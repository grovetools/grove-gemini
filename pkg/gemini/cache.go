package gemini

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"time"

	contextmgr "github.com/mattsolo1/grove-context/pkg/context"
	"google.golang.org/genai"
)

// CacheInfo stores information about cached files.
// It includes the cache ID, name, file hashes for validation,
// the model used, and creation/expiration timestamps.
type CacheInfo struct {
	CacheID          string            `json:"cache_id"`
	CacheName        string            `json:"cache_name"`
	CachedFileHashes map[string]string `json:"cached_file_hashes"`
	Model            string            `json:"model"`
	CreatedAt        time.Time         `json:"created_at"`
	ExpiresAt        time.Time         `json:"expires_at"`
}

// CacheManager manages the cache lifecycle for Gemini API.
// It handles cache creation, validation, and expiration tracking
// for cached content used with the Gemini API.
type CacheManager struct {
	workingDir string
	cacheDir   string
}

// NewCacheManager creates a new cache manager
func NewCacheManager(workingDir string) *CacheManager {
	cacheDir := filepath.Join(workingDir, ".grove", "gemini-cache")
	return &CacheManager{
		workingDir: workingDir,
		cacheDir:   cacheDir,
	}
}

// GetOrCreateCache returns an existing valid cache or creates a new one
func (m *CacheManager) GetOrCreateCache(ctx context.Context, client *Client, model string, coldContextFilePath string, ttl time.Duration, ignoreChanges bool, disableExpiration bool) (*CacheInfo, error) {
	// Check if caching is disabled via grove-context directive
	contextManager := contextmgr.NewManager(m.workingDir)
	shouldDisableCache, err := contextManager.ShouldDisableCache()
	if err != nil {
		// Log warning but continue - don't fail if we can't read the directive
		fmt.Fprintf(os.Stderr, "‚ö†Ô∏è  Warning: Could not check cache directive: %v\n", err)
	}
	
	if shouldDisableCache {
		fmt.Fprintf(os.Stderr, "üö´ Cache disabled by @disable-cache directive\n")
		return nil, nil
	}

	// Check if the cold context file exists
	if _, err := os.Stat(coldContextFilePath); err != nil {
		if os.IsNotExist(err) {
			// No cold context file, return nil (no cache to use)
			return nil, nil
		}
		return nil, fmt.Errorf("checking cold context file: %w", err)
	}

	// Ensure cache directory exists
	if err := os.MkdirAll(m.cacheDir, 0755); err != nil {
		return nil, fmt.Errorf("creating cache directory: %w", err)
	}

	// Generate cache key based on the cold context file path
	cacheKey := generateCacheKey([]string{coldContextFilePath})
	cacheInfoFile := filepath.Join(m.cacheDir, "hybrid_"+cacheKey+".json")

	// Try to load existing cache info
	var cacheInfo CacheInfo
	needNewCache := false

	if data, err := os.ReadFile(cacheInfoFile); err == nil {
		if err := json.Unmarshal(data, &cacheInfo); err == nil {
			fmt.Fprintf(os.Stderr, "üìÅ Found existing cache info\n")

			// Check if cache expired
			if !disableExpiration && time.Now().After(cacheInfo.ExpiresAt) {
				fmt.Fprintf(os.Stderr, "‚è∞ Cache expired at %s\n", cacheInfo.ExpiresAt.Format(time.RFC3339))
				needNewCache = true
			} else if changed, changedFiles := hasFilesChanged(cacheInfo.CachedFileHashes, []string{coldContextFilePath}); changed {
				if ignoreChanges {
					fmt.Fprintf(os.Stderr, "‚ö†Ô∏è  Ignoring file changes and using stale cache due to directive\n")
					return &cacheInfo, nil
				}
				fmt.Fprintf(os.Stderr, "üîÑ Cached files have changed:\n")
				for _, file := range changedFiles {
					fmt.Fprintf(os.Stderr, "   ‚Ä¢ %s\n", file)
				}
				needNewCache = true
			} else {
				validityMessage := "‚úÖ Cache is valid"
				if disableExpiration {
					validityMessage += " (expiration disabled by @no-expire)"
				} else {
					validityMessage += fmt.Sprintf(" until %s", cacheInfo.ExpiresAt.Format(time.RFC3339))
				}
				fmt.Fprintf(os.Stderr, "%s\n", validityMessage)
				return &cacheInfo, nil
			}
		}
	} else {
		fmt.Fprintf(os.Stderr, "üÜï No existing cache found\n")
		needNewCache = true
	}

	// Create new cache if needed
	if needNewCache {
		// First, check if the file is large enough for caching
		content, err := os.ReadFile(coldContextFilePath)
		if err != nil {
			return nil, fmt.Errorf("failed to read %s: %w", coldContextFilePath, err)
		}

		estimatedTokens := estimateTokens(content)
		minTokensForCache := 4096

		if estimatedTokens < minTokensForCache {
			fmt.Fprintf(os.Stderr, "\n‚ö†Ô∏è  Cached context is too small for Gemini caching\n")
			fmt.Fprintf(os.Stderr, "   Estimated tokens: %d (minimum required: %d)\n", estimatedTokens, minTokensForCache)
			fmt.Fprintf(os.Stderr, "   Suggestion: Move all content to hot context (.grove/context) for better performance\n")
			fmt.Fprintf(os.Stderr, "   Proceeding without cache...\n")
			return nil, nil // Return nil to indicate no cache should be used
		}

		fmt.Fprintf(os.Stderr, "\nüì§ Uploading files for cache...\n")
		fmt.Fprintf(os.Stderr, "   Estimated tokens: %d\n", estimatedTokens)

		fileHashes := make(map[string]string)
		var parts []*genai.Part

		// Calculate hash
		hashArray := sha256.Sum256(content)
		hash := hex.EncodeToString(hashArray[:])
		fileHashes[coldContextFilePath] = hash

		// Upload file
		f, err := uploadFile(ctx, client.GetClient(), coldContextFilePath)
		if err != nil {
			return nil, fmt.Errorf("failed to upload %s: %w", coldContextFilePath, err)
		}
		parts = append(parts, genai.NewPartFromURI(f.URI, f.MIMEType))

		// Create cache
		fmt.Fprintf(os.Stderr, "\nüî® Creating cache...\n")
		contents := []*genai.Content{
			genai.NewContentFromParts(parts, genai.RoleUser),
		}

		cacheConfig := &genai.CreateCachedContentConfig{
			Contents: contents,
			TTL:      ttl,
		}

		cache, err := client.GetClient().Caches.Create(ctx, model, cacheConfig)
		if err != nil {
			return nil, fmt.Errorf("failed to create cache: %w", err)
		}

		// Save cache info
		cacheInfo = CacheInfo{
			CacheID:          cache.Name,
			CacheName:        cacheKey,
			CachedFileHashes: fileHashes,
			Model:            model,
			CreatedAt:        time.Now(),
			ExpiresAt:        cache.ExpireTime,
		}

		data, _ := json.MarshalIndent(cacheInfo, "", "  ")
		if err := os.WriteFile(cacheInfoFile, data, 0644); err != nil {
			return nil, fmt.Errorf("failed to save cache info: %w", err)
		}

		fmt.Fprintf(os.Stderr, "  ‚úÖ Cache created: %s\n", cache.Name)
		fmt.Fprintf(os.Stderr, "  üìÖ Expires: %s\n", cache.ExpireTime.Format(time.RFC3339))
	}

	return &cacheInfo, nil
}

// hashFile calculates SHA256 hash of a file
func hashFile(filePath string) (string, error) {
	content, err := os.ReadFile(filePath)
	if err != nil {
		return "", err
	}
	hash := sha256.Sum256(content)
	return hex.EncodeToString(hash[:]), nil
}

// generateCacheKey creates a unique key for a set of files
func generateCacheKey(files []string) string {
	h := sha256.New()
	h.Write([]byte("hybrid_v1"))
	for _, f := range files {
		h.Write([]byte(filepath.Clean(f)))
	}
	return hex.EncodeToString(h.Sum(nil))[:16]
}

// estimateTokens provides a rough estimate of token count for a file
// Using a simple heuristic: ~1 token per 4 characters (common for code/text)
func estimateTokens(content []byte) int {
	return len(content) / 4
}

// hasFilesChanged checks if any files have changed and returns the changed files
func hasFilesChanged(oldHashes map[string]string, files []string) (bool, []string) {
	var changedFiles []string

	for _, file := range files {
		newHash, err := hashFile(file)
		if err != nil {
			changedFiles = append(changedFiles, fmt.Sprintf("%s (error reading file: %v)", file, err))
			continue
		}

		if oldHash, exists := oldHashes[file]; !exists {
			changedFiles = append(changedFiles, fmt.Sprintf("%s (new file)", file))
		} else if oldHash != newHash {
			changedFiles = append(changedFiles, file)
		}
	}

	return len(changedFiles) > 0, changedFiles
}

